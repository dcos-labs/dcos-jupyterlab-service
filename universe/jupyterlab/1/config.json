{
  "type": "object",
  "properties": {
    "service": {
      "type": "object",
      "id": "https://github.com/dcos/examples/tree/master/jupyterlab",
      "title": "Mesosphere Jupyter Service Configuration",
      "description": "Mesospere Jupyter Service Configuration Properties.",
      "properties": {
        "name": {
          "type": "string",
          "title": "Service Name",
          "description": "Unique name for this Mesosphere Jupyter Service instance.",
          "default": "/jupyter-notebook"
        },
        "cmd": {
          "type": "string",
          "title": "Entrypoint",
          "description": "The shell command to use to launch the Mesosphere Jupyter Service.",
          "default": "/usr/local/bin/start.sh ${CONDA_DIR}/bin/jupyter lab --notebook-dir=\"${MESOS_SANDBOX}\""
        },
        "cpus": {
          "type": "number",
          "title": "CPU Allocation",
          "description": "CPU shares to allocate to this service instance - e.g., 2.0",
          "minimum": 1.0,
          "default": 2.0
        },
        "mem": {
          "type": "integer",
          "title": "Memory Allocation",
          "description": "Memory to allocate to the service instance in MiB - e.g., 8192",
          "minimum": 4096,
          "default": 8192
        },
        "gpu_support": {
          "type": "object",
          "title": "Nvidia GPU Configuration",
          "description": "Nvidia GPU Allocation Configuration.",
          "properties": {
            "enabled": {
              "title": "Enable Nvidia GPU Support?",
              "description": "Note: This requires that CUDA 9.0-compatible Nvidia drivers be installed on the DC/OS agents that have GPUs.",
              "type": "boolean",
              "default": false
            },
            "gpus": {
              "type": "integer",
              "title": "Mesosphere Jupyter Service Nvidia GPU Allocation",
              "description": "Number of Nvidia GPUs to allocate to the Mesosphere Jupyter Service.\nNote: GPU support has to be enabled.",
              "minimum": 0,
              "default": 0
            }
          }
        },
        "user": {
          "type": "string",
          "title": "Linux User (Notebook Process Execution Context)",
          "description": "The Linux user account name as whom the Jupyter Notebook will run.",
          "default": "nobody"
        },
        "service_account": {
          "type": "string",
          "title": "Jupyter Notebook Service Account",
          "description": "The DC/OS Service Account to use for Service Authentication (e.g., Apache Spark), or leave empty to use the default.",
          "default": ""
        },
        "service_account_secret": {
          "type": "string",
          "title": "DC/OS Service Account Credential Secret Name",
          "description": "(Optional) Path to the Secret to access the Service Account Credentials to use for DC/OS Service Authentication. This should be left empty unless service authentication is needed.",
          "default": ""
        },
        "force_pull_image": {
          "type": "boolean",
          "title": "Force-Pull Mesosphere Jupyter Service Docker Image?",
          "description": "Always force a pull of the Meossphere Jupyter Service Docker Image?",
          "default": false
        }
      },
      "required": [
        "cpus",
        "mem"
      ]
    },
    "oidc": {
      "type": "object",
      "title": "OpenID Connect (OIDC) Configuration",
      "description": "OpenID Connect Configuration.",
      "properties": {
        "enable_oidc": {
          "type": "boolean",
          "title": "Enable OpenID Connect Authentication for the Mesosphere Jupyter Service?",
          "description": "Enable OpenID Connect Authentication (and optional Authorization)?",
          "default": false
        },
        "oidc_discovery_uri": {
          "type": "string",
          "title": "OIDC Discovery URI",
          "description": "OpenID Connect Discovery URI.",
          "default": "https://keycloak.example.com/auth/realms/notebook/.well-known/openid-configuration"
        },
        "oidc_redirect_uri": {
          "type": "string",
          "title": "OIDC Redirect URI",
          "description": "OpenID Connect Redirect URI.",
          "default": "/oidc-redirect-callback"
        },
        "oidc_client_id": {
          "type": "string",
          "title": "OIDC Client ID",
          "description": "OpenID Connect Client ID.",
          "default": "notebook"
        },
        "oidc_client_secret": {
          "type": "string",
          "title": "OIDC Client Secret",
          "description": "OpenID Connect Client Secret.",
          "default": ""
        },
        "oidc_tls_verify": {
          "type": "boolean",
          "title": "OIDC TLS Verify?",
          "description": "Verify TLS Certificates presented by the OpenID Connect endpoint?",
          "default": false
        },
        "oidc_authorized_email": {
          "type": "string",
          "title": "Email Address to Authorize",
          "description": "(Optional) E-Mail Address to be authorized (e.g., user@example.com), once authenticated.",
          "default": ""
        },
        "oidc_authorized_upn": {
          "type": "string",
          "title": "User Principal Name (UPN) to Authorize",
          "description": "(Optional) Windows Active Directory Federation Services (AD FS) UPN to be authorized (e.g., user007), once authenticated.",
          "default": ""
        },
        "oidc_logout_path": {
          "type": "string",
          "title": "OIDC Logout Path",
          "description": "OpenID Connect Logout Path.",
          "default": "/logmeout"
        },
        "oidc_use_spartan_resolver": {
          "type": "boolean",
          "title": "OIDC: Use DC/OS Highly Available DNS Dispatcher?",
          "description": "Use the DC/OS DNS Dispatcher (Spartan) to resolve OpenID Connect endpoints",
          "default": true
        }
      }
    },
    "spark": {
      "type": "object",
      "id": "https://spark.apache.org/docs/latest/configuration.html",
      "title": "Apache Spark Configuration",
      "description": "Apache Spark Configuration.",
      "properties": {
        "spark_master_url": {
          "type": "string",
          "title": "spark.master",
          "description": "The cluster manager (--master) to connect to.",
          "default": "mesos://zk://zk-1.zk:2181,zk-2.zk:2181,zk-3.zk:2181,zk-4.zk:2181,zk-5.zk:2181/mesos"
        },
        "spark_conf_cores_max": {
          "title": "spark.cores.max",
          "description": "The maximum amount of CPU cores to request for the application from across the cluster.",
          "type": "integer",
          "minimum": 1,
          "default": 5
        },
        "spark_driver_cores": {
          "type": "integer",
          "title": "spark.driver.cores",
          "description": "Number of cores (--driver-cores) to use for the driver process.",
          "minimum": 1,
          "default": 2
        },
        "spark_conf_executor_cores": {
          "title": "spark.executor.cores",
          "description": "The number of cores to use on each executor. Note: This MUST be set to 1 if using TensorFlowOnSpark",
          "type": "integer",
          "minimum": 1,
          "default": 1
        },
        "spark_driver_memory": {
          "type": "string",
          "title": "spark.driver.memory",
          "description": "Amount of memory (--driver-memory) to use for the driver process, i.e. where SparkContext is initialized, in MiB unless otherwise specified (e.g. 1g, 2g). Note: This must be less than 75% of the memory allocation for the Notebook.",
          "default": "6g"
        },
        "spark_conf_executor_memory": {
          "type": "string",
          "title": "spark.executor.memory",
          "description": "spark.executor.memory: Amount of memory to use per executor process, in MiB unless otherwise specified. (e.g. 2g, 8g).",
          "default": "6g"
        },
        "spark_conf_spark_scheduler_min_registered_resources_ratio": {
          "title": "spark.scheduler.minRegisteredResourcesRatio",
          "description": "The minimum ratio of registered resources before the Spark Driver will allocate work to the Spark Executors.",
          "type": "number",
          "minimum": 0.0,
          "maximum": 1.0,
          "default": 1.0
        },
        "spark_driver_java_options": {
          "type": "string",
          "title": "spark.driver.extraJavaOptions",
          "description": "A string of extra JVM options (--driver-java-options) to pass to the driver.",
          "default": "'-server -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/mnt/mesos/sandbox'"
        },
        "spark_conf_executor_java_options": {
          "type": "string",
          "title": "spark.executor.extraJavaOptions",
          "description": "A string of extra JVM options to pass to executors.",
          "default": "'-server -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/mnt/mesos/sandbox'"
        },
        "spark_conf_eventlog_enabled": {
          "type": "boolean",
          "title": "spark.eventLog.enabled",
          "description": "Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.",
          "default": false
        },
        "spark_conf_eventlog_dir": {
          "type": "string",
          "title": "spark.eventLog.dir",
          "description": "Base directory to which Spark events are logged, if spark.eventLog.enabled is true.",
          "default": "hdfs://hdfs/"
        },
        "spark_conf_hadoop_fs_s3a_aws_credentials_provider": {
          "type": "string",
          "enum": [
              "mesos",
              "docker"
          ],
          "id": "https://docs.mesosphere.com/1.12/deploying-services/containerizers/#container-runtime-features",
          "title": "spark.mesos.containerizer",
          "description": "Must be one of 'mesos' or 'docker'. Note: The Docker Containerizer does not support GPUs or File-Based Secrets",
          "default": "mesos"
        },
        "spark_conf_mesos_principal": {
          "type": "string",
          "title": "",
          "description": "Spark Mesos Principal.",
          "default": ""
        },
        "spark_conf_mesos_role": {
          "type": "string",
          "title": "",
          "description": "Spark Mesos Role.",
          "default": ""
        },
        "spark_conf_mesos_driver_labels": {
          "type": "string",
          "title": "spark.mesos.driver.labels",
          "description": "Mesos labels to add to the driver. Labels are free-form key-value pairs. Key-value pairs should be separated by a colon, and commas used to list more than one. If your label includes a colon or comma, you can escape it with a backslash. Ex. key:value,key2:a\\:b.",
          "default": "DCOS_SPACE:"
        },
        "spark_conf_mesos_task_labels": {
          "type": "string",
          "title": "spark.mesos.task.labels",
          "description": "Set the Mesos labels to add to each task. Labels are free-form key-value pairs. Key-value pairs should be separated by a colon, and commas used to list more than one. If your label includes a colon or comma, you can escape it with a backslash. Ex. key:value,key2:a\\:b.",
          "default": "DCOS_SPACE:"
        },
        "spark_conf_executor_krb5_config": {
          "type": "string",
          "title": "spark.executorEnv.KRB5_CONFIG",
          "description": "Location of the krb5.conf file passed via the KRB5_CONFIG environment variable to the Spark Executors.",
          "default": "/mnt/mesos/sandbox/krb5.conf"
        },
        "spark_conf_executor_java_home": {
          "type": "string",
          "title": "spark.executorEnv.JAVA_HOME",
          "description": "Location of the Java runtime passed via the JAVA_HOME environment variable to the Spark Executors.",
          "default": "/opt/jdk"
        },
        "spark_conf_executor_hadoop_hdfs_home": {
          "type": "string",
          "title": "spark.executorEnv.HADOOP_HDFS_HOME",
          "description": "Location of the Hadoop distribution passed via the HADOOP_HDFS_HOME environment variable to the Spark Executors.",
          "default": "/opt/hadoop"
        },
        "spark_conf_executor_hadoop_opts": {
          "type": "string",
          "title": "spark.executorEnv.HADOOP_OPTS",
          "description": "JVM Options to pass to the Hadoop runtime via the HADOOP_OPTS environment variable to the Spark Executors.",
          "default": "spark.executorEnv.HADOOP_OPTS='-Djava.library.path=/opt/hadoop/lib/native -Djava.security.krb5.conf=/mnt/mesos/sandbox/krb5.conf'"
        },
        "spark_conf_mesos_executor_docker_forcepullimage": {
          "type": "boolean",
          "title": "spark.mesos.executor.docker.forcePullImage",
          "description": "Force Mesos agents to pull the image specified in spark.mesos.executor.docker.image.",
          "default": false
        },
        "spark_user": {
          "type": "string",
          "title": "Spark User (Process Execution Context)",
          "description": "The Linux user under which the Spark Executors will run",
          "default": "nobody"
        },
        "start_spark_history_server": {
          "type": "boolean",
          "title": "Start Spark History Server?",
          "description": "Start Spark History Server.",
          "default": false
        },
        "spark_history_fs_logdirectory": {
          "type": "string",
          "title": "Spark EventLog Directory.",
          "description": "Spark EvenLog Directory from which to load events for prior Spark job runs.",
          "default": "hdfs://hdfs/"
        },
        "enable_spark_monitor": {
          "type": "boolean",
          "title": "Enable Spark Monitor?",
          "description": "Enable the Spark Monitor Jupyter Notebook Server Extension to proxy the Spark Driver UI?",
          "default": true
        }
      }
    },
    "storage": {
      "type": "object",
      "title": "Mesosphere Jupyter Service Storage Configuration.",
      "description": "Mesosphere Jupyter Service Storage Configuration.",
      "properties": {
        "s3": {
          "type": "object",
          "title": "S3 Object Store Configuration",
          "description": "S3 Object Store Configuration.",
          "properties": {
            "aws_region": {
              "type": "string",
              "title": "S3: Region",
              "description": "S3 Region.",
              "default": "us-east-1"
            },
            "s3_endpoint": {
              "type": "string",
              "title": "S3: Endpoint",
              "description": "S3 Endpoint.",
              "default": "s3.us-east-1.amazonaws.com"
            },
            "s3_https": {
              "type": "boolean",
              "title": "S3: Enable HTTPS?",
              "description": "Enable connections to S3 over HTTPS?",
              "default": true
            },
            "s3_ssl": {
              "type": "boolean",
              "title": "S3: Verify TLS?",
              "description": "Verify TLS Certificate(s) for the S3 endpoint?",
              "default": true
            }
          }
        },
        "persistence": {
          "type": "object",
          "title": "Local Persistent Storage Configuration",
          "description": "Local Persistent Storage Configuration.",
          "properties": {
            "host_volume_size": {
              "title": "Persistent Volume Size",
              "description": "Size of the local persistent volume in MiB.",
              "type": "integer",
              "default": 4000
            },
            "enable": {
              "title": "Enable Local Persistent Storage?",
              "description": "Enable or disable persistent storage.",
              "type": "boolean",
              "default": false
            }
          }
        }
      }
    },
    "networking": {
      "type": "object",
      "title": "Mesosphere Jupyter Service Networking Configuration",
      "description": "Mesosphere Jupyter Service Networking Configuration.",
      "properties": {
        "cni_support": {
          "type": "object",
          "title": "",
          "description": "Enable Container Networking Interface (CNI) Support.",
          "properties": {
            "enabled": {
              "type": "boolean",
              "description": "Enable CNI support for this Jupyter Notebook instance.",
              "default": true
            }
          }
        },
        "external_access": {
          "type": "object",
          "description": "Enable access from outside the cluster through Marathon-LB.\nNOTE: this connection is unencrypted.",
          "properties": {
            "enabled": {
              "type": "boolean",
              "description": "Enable or disable external access through a public node via Marathon-LB.",
              "default": true
            },
            "external_public_agent_hostname": {
              "type": "string",
              "description": "For external access, DNS to be used for Marathon-LB vHost: For example use your public slave ELB DNS.",
              "default": ""
            }
          }
        }
      }
    },
    "environment": {
      "type": "object",
      "title": "",
      "description": "Additional Configuration Options.",
      "properties": {
        "conda_envs_path": {
          "type": "string",
          "title": "Conda Environment Paths",
          "description": "Conda Environment Paths (CONDA_ENVS_PATH).",
          "default": "/mnt/mesos/sandbox/conda/envs:/opt/conda/envs"
        },
        "conda_pkgs_dir": {
          "type": "string",
          "title": "Conda Package Directory Paths",
          "description": "Conda Package Directory Paths (CONDA_PKGS_DIRS).",
          "default": "/mnt/mesos/sandbox/conda/pkgs:/opt/conda/pkgs"
        },
        "dcos_dir": {
          "type": "string",
          "title": "DC/OS CLI Configuration Directory",
          "description": "DC/OS CLI Config Directory (DCOS_DIR).",
          "default": "/mnt/mesos/sandbox/.dcos"
        },
        "hadoop_conf_dir": {
          "type": "string",
          "title": "Haddop Configuration Directory",
          "description": "Hadoop Configuration Directory (HADOOP_CONF_DIR).",
          "default": "/mnt/mesos/sandbox"
        },
        "home": {
          "type": "string",
          "title": "Home Directory",
          "description": "Your Home Directory (HOME).",
          "default": "/mnt/mesos/sandbox"
        },
        "java_opts": {
          "type": "string",
          "title": "JVM Options",
          "description": "JVM Options (JAVA_OPTS).",
          "default": "'-server -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/mnt/mesos/sandbox'"
        },
        "jupyter_conf_urls": {
          "type": "string",
          "title": "Jupyter Configuration URLs",
          "description": "Jupyter Configuration URLs as comma seperated list of URLs (JUPYTER_CONF_URLS).",
          "default": ""
        },
        "jupyter_config_dir": {
          "type": "string",
          "title": "Jupyter Notebook Configuration Directory",
          "description": "Jupyter Notebook Configuration Directory (JUPYTER_CONFIG_DIR).",
          "default": "/mnt/mesos/sandbox/.jupyter"
        },
        "jupyter_password": {
          "type": "string",
          "title": "Jupyter Notebook Password",
          "description": "Jupyter Notebook Password (JUPYTER_PASSWORD).\n\nDefaults to\"jupyter\" unless deployed into a folder",
          "default": "jupyter"
        },
        "jupyter_runtime_dir": {
          "type": "string",
          "title": "Jupyter Notebook Runtime Directory",
          "description": "Jupyter Notebook Runtime Directory (JUPYTER_RUNTIME_DIR).",
          "default": "/mnt/mesos/sandbox/.local/share/jupyter/runtime"
        },
        "nginx_log_level": {
          "type": "string",
          "enum": [
              "debug",
              "warn",
              "info"
          ],
          "title": "NGINX Log Level",
          "description": "NGINX Log Level (NGINX_LOG_LEVEL).",
          "default": "warn"
        },
        "start_dask_distributed": {
          "type": "boolean",
          "title": "Start Dask Distributed?",
          "description": "Start Dask's Distributed Scheduler?",
          "default": false
        },
        "start_ray_head_node": {
          "type": "boolean",
          "title": "Start Ray Head Node?",
          "description": "Start Ray Head Node?",
          "default": false
        },
        "start_tensorboard": {
          "type": "boolean",
          "title": "Start TensorBoard?",
          "description": "Start TensorBoard?",
          "default": false
        },
        "tensorboard_logdir": {
          "type": "string",
          "title": "TensorBoard Log Directory",
          "description": "TensorBoard Log Directory.",
          "default": "hdfs://hdfs/"
        },
        "term": {
          "type": "string",
          "title": "Terminal (TTY) Type",
          "description": "Unix Terminal Type.",
          "default": "xterm-256color"
        }
      }
    }
  }
}
